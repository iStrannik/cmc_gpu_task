Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1286075: <mpiexec -n 2 ./main_mpi_gpu_160_180> in cluster <MSUCluster> Done

Job <mpiexec -n 2 ./main_mpi_gpu_160_180> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-skmodel24-619-07> in cluster <MSUCluster> at Thu Dec 19 10:06:05 2024
Job was executed on host(s) <2*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <short>, as user <edu-cmc-skmodel24-619-07> in cluster <MSUCluster> at Thu Dec 19 10:06:06 2024
</home_edu/edu-cmc-skmodel24-619/edu-cmc-skmodel24-619-07> was used as the home directory.
</home_edu/edu-cmc-skmodel24-619/edu-cmc-skmodel24-619-07> was used as the working directory.
Started at Thu Dec 19 10:06:06 2024
Terminated at Thu Dec 19 10:08:36 2024
Results reported at Thu Dec 19 10:08:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpiexec -n 2 ./main_mpi_gpu_160_180
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   300.75 sec.
    Max Memory :                                 1443 MB
    Average Memory :                             915.67 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                21
    Run time :                                   150 sec.
    Turnaround time :                            151 sec.

The output (if any) follows:

0 2
1 2
436649 148.769505
time_AW = 13.280702
time_update_to_host_r = 11.021204
time_scalar_r_r = 41.174089
time_asycn_send = 1.112356
time_update_to_device_r = 9.253544
time_Ar = 13.440612
time_scalar_Ar_r = 40.895323
time_tau = 5.142855
time_new_w = 11.407223


PS:

Read file <error_main_mpi_gpu_160_180_2> for stderr output of this job.

Sender: LSF System <lsfadmin@polus-c3-ib.bmc.hpc.cs.msu.ru>
Subject: Job 1286476: <mpiexec -n 2 ./main_mpi_gpu_160_180> in cluster <MSUCluster> Done

Job <mpiexec -n 2 ./main_mpi_gpu_160_180> was submitted from host <polus-ib.bmc.hpc.cs.msu.ru> by user <edu-cmc-skmodel24-619-07> in cluster <MSUCluster> at Thu Dec 19 15:25:23 2024
Job was executed on host(s) <2*polus-c3-ib.bmc.hpc.cs.msu.ru>, in queue <short>, as user <edu-cmc-skmodel24-619-07> in cluster <MSUCluster> at Thu Dec 19 15:44:13 2024
</home_edu/edu-cmc-skmodel24-619/edu-cmc-skmodel24-619-07> was used as the home directory.
</home_edu/edu-cmc-skmodel24-619/edu-cmc-skmodel24-619-07> was used as the working directory.
Started at Thu Dec 19 15:44:13 2024
Terminated at Thu Dec 19 15:44:15 2024
Results reported at Thu Dec 19 15:44:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
mpiexec -n 2 ./main_mpi_gpu_160_180
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.67 sec.
    Max Memory :                                 1 MB
    Average Memory :                             1.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              1
    Max Threads :                                1
    Run time :                                   1 sec.
    Turnaround time :                            1132 sec.

The output (if any) follows:

0 2
1 2
2 0.171042
time_AW = 0.000273
time_update_to_host_r = 0.000102
time_scalar_r_r = 0.000194
time_asycn_send = 0.000066
time_update_to_device_r = 0.000060
time_Ar = 0.000077
time_scalar_Ar_r = 0.000113
time_tau = 0.000133
time_new_w = 0.000069


PS:

Read file <error_main_mpi_gpu_160_180_2> for stderr output of this job.

